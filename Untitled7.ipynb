{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEzPU-YUyFSx",
        "outputId": "d3eb916f-9b26-4ae3-f801-461d935b51dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Vo0s87aygMJ",
        "outputId": "134c6c30-5002-40eb-f0f9-467796e8406d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uninstalling all potentially conflicting packages...\\n",
            "\\u001b[33mWARNING: Skipping xformers as it is not installed.\\u001b[0m\\u001b[33m\\n",
            "\\u001b[0m\\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\\u001b[0m\\u001b[33m\\n",
            "\\u001b[0mInstalling PyTorch, torchvision, and torchaudio (latest cu121)...\\n",
            "\\u001b[2K     \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m780.5/780.5 MB\\u001b[0m \\u001b[31m808.0 kB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[2K     \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m7.3/7.3 MB\\u001b[0m \\u001b[31m92.0 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[2K     \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m3.4/3.4 MB\\u001b[0m \\u001b[31m69.5 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[2K     \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m23.7/23.7 MB\\u001b[0m \\u001b[31m77.2 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[2K     \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m823.6/823.6 kB\\u001b[0m \\u001b[31m41.4 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[2K     \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m14.1/14.1 MB\\u001b[0m \\u001b[31m100.6 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[2K     \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m664.8/664.8 MB\\u001b[0m \\u001b[31m829.9 kB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[2K     \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m410.6/410.6 MB\\u001b[0m \\u001b[31m3.7 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[2K     \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m121.6/121.6 MB\\u001b[0m \\u001b[31m8.5 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[2K     \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m56.5/56.5 MB\\u001b[0m \\u001b[31m14.6 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[2K     \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m124.2/124.2 MB\\u001b[0m \\u001b[31m8.0 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[2K     \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m196.0/196.0 MB\\u001b[0m \\u001b[31m1.8 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[2K     \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m99.1/99.1 kB\\u001b[0m \\u001b[31m6.3 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[2K     \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m209.5/209.5 MB\\u001b[0m \\u001b[31m2.2 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[?25h\\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\\n",
            "timm 1.0.15 requires huggingface_hub, which is not installed.\\u001b[0m\\u001b[31m\\n",
            "\\u001b[0mInstalling xformers (latest compatible)...\\n",
            "\\u001b[2K   \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m15.3/15.3 MB\\u001b[0m \\u001b[31m92.7 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[?25hInstalling bitsandbytes...\\n",
            "\\u001b[2K   \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m67.0/67.0 MB\\u001b[0m \\u001b[31m10.0 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[?25hInstalling latest diffusers, transformers, accelerate, peft, huggingface_hub...\\n",
            "\\u001b[2K   \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m3.6/3.6 MB\\u001b[0m \\u001b[31m48.1 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[2K   \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m10.5/10.5 MB\\u001b[0m \\u001b[31m92.8 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[2K   \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m362.1/362.1 kB\\u001b[0m \\u001b[31m22.3 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[2K   \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m411.1/411.1 kB\\u001b[0m \\u001b[31m25.3 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[2K   \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m512.1/512.1 kB\\u001b[0m \\u001b[31m27.7 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[2K   \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m5.2/5.2 MB\\u001b[0m \\u001b[31m94.7 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[?25hInstalling other dependencies...\\n",
            "\\u001b[2K   \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m54.2/54.2 MB\\u001b[0m \\u001b[31m15.6 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[2K   \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m323.3/323.3 kB\\u001b[0m \\u001b[31m15.9 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[2K   \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m290.4/290.4 kB\\u001b[0m \\u001b[31m12.8 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[2K   \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m95.2/95.2 kB\\u001b[0m \\u001b[31m4.7 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[2K   \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m11.5/11.5 MB\\u001b[0m \\u001b[31m79.8 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[2K   \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m72.0/72.0 kB\\u001b[0m \\u001b[31m4.3 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[2K   \\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\\u001b[0m \\u001b[32m62.5/62.5 kB\\u001b[0m \\u001b[31m4.1 MB/s\\u001b[0m eta \\u001b[36m0:00:00\\u001b[0m\\n",
            "\\u001b[?25h\\n",
            "--- ALL INSTALLATIONS ATTEMPTED ---\\n",
            "Please RESTART RUNTIME (Runtime > Restart runtime) and then run subsequent cells.\\n"
          ]
        }
      ],
      "source": [
        "# --- REVISED INSTALLATION (Attempt 14: Trying Latest Compatible Versions) ---\\n",
        "\\n",
        "# 1. Uninstall *all* potentially conflicting packages.\\n",
        "print(\"Uninstalling all potentially conflicting packages...\")\\n",
        "!pip uninstall -y -q torch torchvision torchaudio xformers bitsandbytes diffusers transformers accelerate sentence-transformers peft huggingface_hub\\n",
        "\\n",
        "# 2. Install PyTorch with the latest available CUDA 12.1 compatible version\\n",
        "#    Let pip resolve to the highest available that matches Colab's CUDA\\n",
        "print(\"Installing PyTorch, torchvision, and torchaudio (latest cu121)...\")\\n",
        "!pip install -qqq torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\\n",
        "\\n",
        "# 3. Install xformers (latest compatible with PyTorch installed above)\\n",
        "print(\"Installing xformers (latest compatible)...\")\\n",
        "!pip install -qqq xformers --index-url https://download.pytorch.org/whl/cu121\\n",
        "\\n",
        "# 4. Install bitsandbytes\\n",
        "print(\"Installing bitsandbytes...\")\\n",
        "!pip install -qqq bitsandbytes\\n",
        "\\n",
        "# 5. Install the very latest diffusers, transformers, accelerate, peft, huggingface_hub\\n",
        "#    This assumes they are now compatible with each other and with the latest PyTorch cu121.\\n",
        "print(\"Installing latest diffusers, transformers, accelerate, peft, huggingface_hub...\")\\n",
        "!pip install -qqq diffusers==0.27.1 transformers accelerate peft huggingface_hub==0.20.3\\n",
        "\\n",
        "# 6. Install other dependencies\\n",
        "print(\"Installing other dependencies...\")\\n",
        "!pip install -qqq Pillow numpy matplotlib scikit-learn opencv-python gradio controlnet_aux python-dotenv\\n",
        "\\n",
        "print(\"\\n--- ALL INSTALLATIONS ATTEMPTED ---\")\\n",
        "print(\"Please RESTART RUNTIME (Runtime > Restart runtime) and then run subsequent cells.\")\\n",
        "\\n",
        "# ... existing code ...\\n",
        "\\n",
        "# --- Hugging Face Login Cell ---\\n",
        "from dotenv import load_dotenv\\n",
        "import os\\n",
        "\\n",
        "# Load environment variables from .env file\\n",
        "load_dotenv()\\n",
        "\\n",
        "# Get token from environment variable\\n",
        "HUGGINGFACE_TOKEN = os.getenv('HUGGINGFACE_TOKEN')\\n",
        "if not HUGGINGFACE_TOKEN:\\n",
        "    raise ValueError(\"HUGGINGFACE_TOKEN not found in .env file\")\\n",
        "\\n",
        "from huggingface_hub import login\\n",
        "login(token=HUGGINGFACE_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-fsuQg2UzWm-"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\\n",
        "import os\\n",
        "\\n",
        "# Load environment variables from .env file\\n",
        "load_dotenv()\\n",
        "\\n",
        "# Get token from environment variable\\n",
        "HUGGINGFACE_TOKEN = os.getenv('HUGGINGFACE_TOKEN')\\n",
        "if not HUGGINGFACE_TOKEN:\\n",
        "    raise ValueError(\"HUGGINGFACE_TOKEN not found in .env file\")\\n",
        "\\n",
        "from huggingface_hub import login\\n",
        "login(token=HUGGINGFACE_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsbQT23dzsvT",
        "outputId": "5623e822-c045-4f1a-8945-be35bb852a94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paths defined:\\n",
            "  Instance Data: /content/drive/MyDrive/mysketches/examples of doodles\\n",
            "  Output: /content/drive/MyDrive/SketchPoemAI/finetuned_model\\n"
          ]
        }
      ],
      "source": [
        "import os # Make sure os is imported here\\n",
        "\\n",
        "# Define paths\\n",
        "output_dir = \"/content/drive/MyDrive/SketchPoemAI/finetuned_model\"\\n",
        "instance_data_dir = \"/content/drive/MyDrive/mysketches/examples of doodles\" # Your sketch directory\\n",
        "instance_prompt = \"a sksart sketch\" # Your unique style token + class prompt\\n",
        "class_data_dir = \"/content/drive/MyDrive/SketchPoemAI/class_images\" # For regularization images\\n",
        "class_prompt = \"a sketch\" # General class prompt\\n",
        "\\n",
        "# Create directories (if not already created)\\n",
        "os.makedirs(output_dir, exist_ok=True)\\n",
        "os.makedirs(class_data_dir, exist_ok=True)\\n",
        "\\n",
        "print(f\"Paths defined:\\n  Instance Data: {instance_data_dir}\\n  Output: {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZgDbf1Ez1EZ"
      },
      "outputs": [],
      "source": [
        "# --- Optional: BLIP Automated Captioning Cell ---\\n",
        "# --- I run it previosuly and it generated text files for me , so no need to run it .\\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\\n",
        "from PIL import Image\\n",
        "import os\\n",
        "import torch\\n",
        "\\n",
        "# No need to redefine instance_data_dir here if it's defined in the previous cell.\\n",
        "# Just make sure the previous cell has been run!\\n",
        "\\n",
        "print(\"Loading BLIP model for captioning...\")\\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(\"cuda\")\\n",
        "print(\"BLIP model loaded.\")\\n",
        "\\n",
        "print(f\"\\nStarting captioning process for images in: {instance_data_dir}\")\\n",
        "processed_count = 0\\n",
        "skipped_count = 0\\n",
        "error_count = 0\\n",
        "\\n",
        "# Loop through all files in your sketch directory\\n",
        "for filename in os.listdir(instance_data_dir):\\n",
        "    # ... (rest of your BLIP code remains the same) ...\\n",
        "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.webp')):\\n",
        "        image_path = os.path.join(instance_data_dir, filename)\\n",
        "        text_path = os.path.join(instance_data_dir, os.path.splitext(filename)[0] + \".txt\")\\n",
        "\\n",
        "        if os.path.exists(text_path):\\n",
        "            skipped_count += 1\\n",
        "            continue\\n",
        "\\n",
        "        try:\\n",
        "            raw_image = Image.open(image_path).convert(\"RGB\")\\n",
        "            inputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\")\\n",
        "            out = model.generate(**inputs, max_length=50, num_beams=5)\\n",
        "            caption = processor.decode(out[0], skip_special_tokens=True)\\n",
        "\\n",
        "            final_caption = f\"{instance_prompt}, {caption}\"\\n",
        "\\n",
        "            with open(text_path, \"w\") as f:\\n",
        "                f.write(final_caption)\\n",
        "\\n",
        "            print(f\"Caption for {filename}: {final_caption}\")\\n",
        "            processed_count += 1\\n",
        "\\n",
        "        except Exception as e:\\n",
        "            print(f\"Error processing {filename}: {e}\")\\n",
        "            error_count += 1\\n",
        "\\n",
        "del model\\n",
        "del processor\\n",
        "if torch.cuda.is_available():\\n",
        "    torch.cuda.empty_cache()\\n",
        "    print(\"Cleared GPU cache.\")\\n",
        "\\n",
        "print(f\"\\n--- Captioning Summary ---\")\\n",
        "print(f\"Processed: {processed_count} images\")\\n",
        "print(f\"Skipped (caption already existed): {skipped_count} images\")\\n",
        "print(f\"Errors: {error_count} images\")\\n",
        "print(\"\\nRemember to manually review and refine the generated captions for accuracy and style adherence!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCHX1L9lz1zv",
        "outputId": "7a55f444-965d-4b66-beb1-21b160039dca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\n",
            "Starting training with runwayml/stable-diffusion-v1-5 and LoRA...\\n",
            "Dataset path: /content/drive/MyDrive/mysketches/examples of doodles\\n",
            "Instance prompt: 'a sksart sketch'\\n",
            "Output directory: /content/drive/MyDrive/SketchPoemAI/finetuned_model\\n",
            "Max training steps: 5000\\n",
            "Attempting to download train_dreambooth_lora.py script to: /content/train_dreambooth_lora.py\\n",
            "From URL: https://raw.githubusercontent.com/huggingface/diffusers/v0.27.1/examples/dreambooth/train_dreambooth_lora.py\\n",
            "/content\\n",
            "Current working directory before download: /content\\n",
            "Successfully downloaded script: /content/train_dreambooth_lora.py\\n",
            "Current working directory for accelerate launch: /content\\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\\n",
            "\\t`--num_processes` was set to a value of `1`\\n",
            "\\t`--num_machines` was set to a value of `1`\\n",
            "\\t`--mixed_precision` was set to a value of `'no'`\\n",
            "\\t`--dynamo_backend` was set to a value of `'no'`\\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\\n",
            "Traceback (most recent call last):\\n",
            "  File \"/content/train_dreambooth_lora.py\", line 46, in <module>\\n",
            "    import diffusers\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/diffusers/__init__.py\", line 5, in <module>\\n",
            "    from .utils import (\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/diffusers/utils/__init__.py\", line 38, in <module>\\n",
            "    from .dynamic_modules_utils import get_class_from_dynamic_module\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/diffusers/utils/dynamic_modules_utils.py\", line 28, in <module>\\n",
            "    from huggingface_hub import cached_download, hf_hub_download, model_info\\n",
            "ImportError: cannot import name 'cached_download' from 'huggingface_hub' (/usr/local/lib/python3.11/dist-packages/huggingface_hub/__init__.py)\\n",
            "Traceback (most recent call last):\\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\\n",
            "    sys.exit(main())\\n",
            "             ^^^^^^\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/accelerate_cli.py\", line 50, in main\\n",
            "    args.func(args)\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/launch.py\", line 1198, in launch_command\\n",
            "    simple_launcher(args)\\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/launch.py\", line 785, in simple_launcher\\n",
            "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\\n",
            "subprocess.CalledProcessError: Command '['/usr/bin/python3', '/content/train_dreambooth_lora.py', '--pretrained_model_name_or_path=runwayml/stable-diffusion-v1-5', '--instance_data_dir=/content/drive/MyDrive/mysketches/examples of doodles', '--instance_prompt=a sksart sketch', '--output_dir=/content/drive/MyDrive/SketchPoemAI/finetuned_model', '--mixed_precision=fp16', '--resolution=512', '--train_batch_size=1', '--gradient_accumulation_steps=1', '--learning_rate=0.0001', '--lr_scheduler=cosine', '--lr_warmup_steps=100', '--max_train_steps=5000', '--checkpointing_steps=1000', '--seed=42', '--with_prior_preservation', '--class_data_dir=/content/drive/MyDrive/SketchPoemAI/class_images', '--class_prompt=a sketch', '--num_class_images=200', '--validation_prompt=a sksart sketch of a tree']' returned non-zero exit status 1.\\n",
            "\\n",
            "Model training complete. LoRA weights saved to: /content/drive/MyDrive/SketchPoemAI/finetuned_model\\n",
            "You can now proceed to the Inference Pipeline cell to generate sketches from poems.\\n"
          ]
        }
      ],
      "source": [
        "# --- Training Pipeline Cell (Final attempt to get it running - FULL CODE) ---\\n",
        "\\n",
        "import os # Ensure os is imported here for file system operations\\n",
        "\\n",
        "# --- Training Parameters ---\\n",
        "model_name = \"runwayml/stable-diffusion-v1-5\" # Base model\\n",
        "resolution = 512\\n",
        "train_batch_size = 1\\n",
        "gradient_accumulation_steps = 1\\n",
        "learning_rate = 1e-4\\n",
        "lr_scheduler = \"cosine\"\\n",
        "lr_warmup_steps = 100\\n",
        "max_train_steps = 5000\\n",
        "save_steps = 1000\\n",
        "seed = 42\\n",
        "\\n",
        "# These variables should be defined from your \"Define Paths & Create Directories\" cell (Cell [5]).\\n",
        "# They are included here as comments just for reference.\\n",
        "\\n",
        "print(f\"\\nStarting training with {model_name} and LoRA...\")\\n",
        "print(f\"Dataset path: {instance_data_dir}\") # This variable should now be defined\\n",
        "print(f\"Instance prompt: '{instance_prompt}'\") # This variable should now be defined\\n",
        "print(f\"Output directory: {output_dir}\") # This variable should now be defined\\n",
        "print(f\"Max training steps: {max_train_steps}\")\\n",
        "\\n",
        "\\n",
        "# --- CRITICAL: Ensure the script downloads and is found ---\\n",
        "script_url = \"https://raw.githubusercontent.com/huggingface/diffusers/v0.27.1/examples/dreambooth/train_dreambooth_lora.py\"\\n",
        "script_local_path = \"/content/train_dreambooth_lora.py\" # The desired path for the downloaded script\\n",
        "\\n",
        "print(f\"Attempting to download train_dreambooth_lora.py script to: {script_local_path}\")\\n",
        "print(f\"From URL: {script_url}\")\\n",
        "\\n",
        "# Ensure we are in /content directory before downloading, so the script goes to the correct place\\n",
        "%cd /content\\n",
        "print(f\"Current working directory before download: {os.getcwd()}\")\\n",
        "\\n",
        "# Use !wget to download the script. -q for quiet, -O to specify output file.\\n",
        "!wget -q -O {script_local_path} {script_url}\\n",
        "\\n",
        "# Verify the script was downloaded\\n",
        "if os.path.exists(script_local_path):\\n",
        "    print(f\"Successfully downloaded script: {script_local_path}\")\\n",
        "else:\\n",
        "    print(f\"ERROR: Script not found after download attempt at {script_local_path}. Please check the URL or your network connection.\")\\n",
        "    raise FileNotFoundError(f\"Training script not found: {script_local_path}\")\\n",
        "\\n",
        "print(f\"Current working directory for accelerate launch: {os.getcwd()}\")\\n",
        "\\n",
        "\\n",
        "# --- Command to run the DreamBooth LoRA training script ---\\n",
        "# We directly execute the downloaded script using its full path ({script_local_path})\\n",
        "!accelerate launch {script_local_path} \\\n",
        "    --pretrained_model_name_or_path=\"$model_name\" \\\n",
        "    --instance_data_dir=\"$instance_data_dir\" \\\n",
        "    --instance_prompt=\"$instance_prompt\" \\\n",
        "    --output_dir=\"$output_dir\" \\\n",
        "    --mixed_precision=\"fp16\" \\\n",
        "    --resolution=$resolution \\\n",
        "    --train_batch_size=$train_batch_size \\\n",
        "    --gradient_accumulation_steps=$gradient_accumulation_steps \\\n",
        "    --learning_rate=$learning_rate \\\n",
        "    --lr_scheduler=$lr_scheduler \\\n",
        "    --lr_warmup_steps=$lr_warmup_steps \\\n",
        "    --max_train_steps=$max_train_steps \\\n",
        "    --checkpointing_steps=$save_steps \\\n",
        "    --seed=$seed \\\n",
        "    --with_prior_preservation --class_data_dir=\"$class_data_dir\" --class_prompt=\"$class_prompt\" \\\n",
        "    --num_class_images=200 \\\n",
        "    --validation_prompt=\"a sksart sketch of a tree\" \\\n",
        "\\n",
        "print(f\"\\nModel training complete. LoRA weights saved to: {output_dir}\")\\n",
        "print(\"You can now proceed to the Inference Pipeline cell to generate sketches from poems.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXPeMbKM_WRX",
        "outputId": "1216636f-3304-423d-f452-55acd969b8ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No checkpoint folders found in /content/drive/MyDrive/SketchPoemAI/finetuned_model.\\n",
            "ERROR: LoRA model weights not found. Cannot proceed with inference.\\n",
            "Please ensure your training process completed and saved the weights to the correct 'output_dir'.\\n"
          ]
        }
      ],
      "source": [
        "# --- Inference Pipeline: Cell 1 - Load the Fine-tuned Model ---\\n",
        "\\n",
        "from diffusers import StableDiffusionPipeline\\n",
        "import torch\\n",
        "import os\\n",
        "\\n",
        "# Define the base model (should match the one you used for training)\\n",
        "base_model_name = \"runwayml/stable-diffusion-v1-5\"\\n",
        "\\n",
        "# Define the path to your fine-tuned LoRA weights\\n",
        "# The training script saves LoRA weights in a subfolder like 'checkpoint-LAST_CHECKPOINT_NUMBER'\\n",
        "# This code will automatically find the latest checkpoint.\\n",
        "\\n",
        "output_dir = \"/content/drive/MyDrive/SketchPoemAI/finetuned_model\" # This variable should be defined from your earlier 'Define Paths' cell\\n",
        "\\n",
        "latest_checkpoint_path = None\\n",
        "if os.path.exists(output_dir):\\n",
        "    # Find all checkpoint folders\\n",
        "    checkpoint_folders = [f for f in os.listdir(output_dir) if f.startswith('checkpoint-')]\\n",
        "    if checkpoint_folders:\\n",
        "        # Get the highest numbered checkpoint folder (e.g., checkpoint-5000)\\n",
        "        latest_checkpoint_folder = max(checkpoint_folders, key=lambda x: int(x.split('-')[1]))\\n",
        "        latest_checkpoint_path = os.path.join(output_dir, latest_checkpoint_folder)\\n",
        "        # Construct the full path to the weights file\\n",
        "        lora_model_path = os.path.join(latest_checkpoint_path, \"pytorch_lora_weights.safetensors\")\\n",
        "    else:\\n",
        "        print(f\"No checkpoint folders found in {output_dir}.\")\\n",
        "        lora_model_path = None\\n",
        "else:\\n",
        "    print(f\"Output directory {output_dir} does not exist. Please check your path.\")\\n",
        "    lora_model_path = None\\n",
        "\\n",
        "# Proceed only if the LoRA weights path is valid\\n",
        "if lora_model_path and os.path.exists(lora_model_path):\\n",
        "    print(f\"Found LoRA weights at: {lora_model_path}\")\\n",
        "    print(\"Loading base Stable Diffusion pipeline...\")\\n",
        "    # Load the base model; it will download if not cached\\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(base_model_name, torch_dtype=torch.float16)\\n",
        "\\n",
        "    print(\"Loading LoRA weights into the pipeline...\")\\n",
        "    # Load your custom LoRA weights\\n",
        "    pipe.load_lora_weights(os.path.dirname(lora_model_path), weight_name=os.path.basename(lora_model_path))\\n",
        "\\n",
        "    # Move the entire model to the GPU for inference\\n",
        "    pipe.to(\"cuda\")\\n",
        "    print(\"Fine-tuned model loaded successfully for inference!\")\\n",
        "else:\\n",
        "    print(\"ERROR: LoRA model weights not found. Cannot proceed with inference.\")\\n",
        "    print(\"Please ensure your training process completed and saved the weights to the correct 'output_dir'.\")\\n",
        "    pipe = None # Set pipe to None if loading failed, to prevent further errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqHHOi89Akpa",
        "outputId": "565c0c1d-d07c-47e2-f0da-df52d0841e50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking contents of: /content/drive/MyDrive/SketchPoemAI/finetuned_model\\n",
            "\\n",
            "Directory is empty.\\n",
            "\\n",
            "Please verify the actual contents of your Google Drive.\\n"
          ]
        }
      ],
      "source": [
        "# --- Debugging: Check Contents of Output Directory ---\\n",
        "import os\\n",
        "\\n",
        "output_dir = \"/content/drive/MyDrive/SketchPoemAI/finetuned_model\"\\n",
        "\\n",
        "print(f\"Checking contents of: {output_dir}\")\\n",
        "if os.path.exists(output_dir):\\n",
        "    contents = os.listdir(output_dir)\\n",
        "    if contents:\\n",
        "        print(\"Contents found:\")\\n",
        "        for item in contents:\\n",
        "            print(f\"- {item}\")\\n",
        "\\n",
        "        checkpoint_folders = [f for f in contents if f.startswith('checkpoint-')]\\n",
        "        if checkpoint_folders:\\n",
        "            print(f\"\\nDetected checkpoint folders: {checkpoint_folders}\")\\n",
        "            latest_checkpoint_folder = max(checkpoint_folders, key=lambda x: int(x.split('-')[1]))\\n",
        "            potential_lora_path = os.path.join(output_dir, latest_checkpoint_folder, \"pytorch_lora_weights.safetensors\")\\n",
        "            print(f\"Looking for LoRA weights at: {potential_lora_path}\")\\n",
        "            if os.path.exists(potential_lora_path):\\n",
        "                print(\"\\nSUCCESS: LoRA weights file EXISTS at this path!\")\\n",
        "                print(\"You can proceed to run the 'Inference Pipeline: Cell 1 - Load the Fine-tuned Model' again.\")\\n",
        "            else:\\n",
        "                print(\"\\nERROR: LoRA weights file DOES NOT EXIST at the expected path inside the checkpoint folder.\")\\n",
        "                print(\"Double-check the file name inside your checkpoint folder.\")\\n",
        "        else:\\n",
        "            print(\"\\nERROR: No folders starting with 'checkpoint-' found.\")\\n",
        "    else:\\n",
        "        print(\"\\nDirectory is empty.\")\\n",
        "else:\\n",
        "    print(f\"\\nERROR: Directory {output_dir} does NOT exist.\")\\n",
        "\\n",
        "print(\"\\nPlease verify the actual contents of your Google Drive.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "new_cell_1_nlp"
      },
      "outputs": [],
      "source": [
        "# --- Inference Pipeline: Cell 2 - Poem Understanding (NLP) and Prompt Construction ---\\n",
        "\\n",
        "from transformers import pipeline\\n",
        "import random\\n",
        "from collections import Counter # Import Counter for keyword extraction\\n",
        "\\n",
        "# Load a basic sentiment analyzer. This is a robust model for emotion detection.\\n",
        "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\")\\n",
        "\\n",
        "# 'instance_prompt' should be defined from your earlier 'Define Paths' cell (e.g., \"a sksart sketch\").\\n",
        "# We use it here to ensure the generated images adhere to your style.\\n",
        "# instance_prompt = \"a sksart sketch\"\\n",
        "\\n",
        "def analyze_poem_for_prompt(poem_text, style_token=instance_prompt):\\n",
        "    \"\"\"\\n",
        "    Analyzes a poem for sentiment and key concepts, then constructs a Stable Diffusion prompt.\\n",
        "    \"\"\"\\n",
        "    if not poem_text.strip(): # Handle empty poem input\\n",
        "        return \"\", \"\" # Return empty prompts if no poem text\\n",
        "\\n",
        "    sentiment_result = sentiment_analyzer(poem_text)[0]\\n",
        "    sentiment_label = sentiment_result['label'] # e.g., 'POSITIVE', 'NEGATIVE', 'NEUTRAL'\\n",
        "    \\n",
        "    # Simple keyword extraction: split words, filter short/non-alphabetic, count frequency\\n",
        "    words = [word.lower() for word in poem_text.replace('\\n', ' ').split() if len(word) > 3 and word.isalpha()]\\n",
        "    \\n",
        "    # Map sentiment to abstract visual concepts. These influence the overall mood.\\n",
        "    abstract_concepts = \"\"\\n",
        "    if sentiment_label == \"POSITIVE\":\\n",
        "        abstract_concepts = \"light, flowing shapes, harmony, ascent, joyful, bright, vibrant\"\\n",
        "    elif sentiment_label == \"NEGATIVE\":\\n",
        "        abstract_concepts = \"jagged lines, heavy forms, darkness, descent, fragmented, melancholic, chaotic, somber\"\\n",
        "    elif sentiment_label == \"NEUTRAL\":\\n",
        "        abstract_concepts = \"balanced forms, subtle shifts, contemplative, stillness, ethereal, dreamlike, serene\"\\n",
        "    \\n",
        "    # Extract a few literal keywords from the poem to add specificity, filtering out common words\\n",
        "    word_counts = Counter(words)\\n",
        "    # Filter common English words that might not add visual meaning\\n",
        "    common_filler_words = {\"the\", \"and\", \"that\", \"with\", \"from\", \"for\", \"are\", \"was\", \"has\", \"have\", \"you\", \"they\", \"this\", \"but\", \"not\", \"its\", \"her\", \"his\", \"their\", \"our\", \"there\", \"when\", \"where\", \"what\", \"which\", \"who\", \"whom\", \"whose\", \"why\", \"how\", \"then\", \"than\", \"more\", \"most\", \"each\", \"some\", \"such\", \"into\", \"onto\", \"upon\", \"about\", \"above\", \"across\", \"after\", \"again\", \"against\", \"among\", \"around\", \"at\", \"before\", \"behind\", \"below\", \"beneath\", \"beside\", \"between\", \"beyond\", \"down\", \"during\", \"except\", \"inside\", \"like\", \"near\", \"off\", \"on\", \"out\", \"outside\", \"over\", \"past\", \"through\", \"under\", \"until\", \"up\", \"while\", \"within\", \"without\", \"will\", \"would\", \"could\", \"should\", \"might\", \"must\", \"can\", \"may\"}\\n",
        "    \\n",
        "    top_keywords = [\\n",
        "        word for word, count in word_counts.most_common(5) # Get top 5 most common words\\n",
        "        if word not in common_filler_words and word.isalpha() # Filter out filler words and non-alphabetic\\n",
        "    ]\\n",
        "    \\n",
        "    # Construct the primary positive prompt\\n",
        "    # Start with your style token, then abstract concepts, then keywords for specificity\\n",
        "    prompt = f\"{style_token} of {abstract_concepts}\"\\n",
        "    if top_keywords:\\n",
        "        prompt += f\", depicting {', '.join(top_keywords)}\"\\n",
        "    \\n",
        "    # Add general style descriptors that define your \"sketch\" aesthetic\\n",
        "    prompt += \", expressive lines, abstract visual storytelling, hand-drawn, ink drawing, charcoal, graphite pencil, high contrast, deep shadows, minimalist, evocative\"\\n",
        "    \\n",
        "    # Define negative prompt to avoid undesirable elements\\n",
        "    negative_prompt = \"text, words, realistic, deformed, blurry, ugly, distorted, low quality, human figures, cartoon, photography, watermark, signature, cluttered, repetitive patterns, cartoon, anime, 3d, digital art\"\\n",
        "    \\n",
        "    return prompt, negative_prompt\\n",
        "\\n",
        "# You can test the prompt generation here before using the Gradio UI\\n",
        "# test_poem = \"\"\"\\n",
        "# The forgotten echoes of a dream,\\n",
        "# A fractured silence, a weeping stream.\\n",
        "# Through broken glass, new light now gleams,\\n",
        "# Hope's fragile tendrils, in silver beams.\\n",
        "# \"\"\"\\n",
        "# generated_prompt, generated_neg_prompt = analyze_poem_for_prompt(test_poem)\\n",
        "# print(f\"\\n--- Test Prompt ---\")\\n",
        "# print(f\"Positive: {generated_prompt}\")\\n",
        "# print(f\"Negative: {generated_neg_prompt}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "new_cell_2_gradio"
      },
      "outputs": [],
      "source": [
        "# --- Inference Pipeline: Cell 3 - Interactive Tool (Gradio UI) ---\\n",
        "\\n",
        "import gradio as gr\\n",
        "import torch\\n",
        "import random\\n",
        "from PIL import Image\\n",
        "\\n",
        "# Ensure 'pipe' (your loaded model) is available from Cell 1.\\n",
        "# If Cell 1 failed or wasn't run, 'pipe' won't exist or will be None.\\n",
        "if 'pipe' not in locals() or pipe is None:\\n",
        "    print(\"Model 'pipe' not loaded. Please run 'Cell 1: Load the Fine-tuned Model' first.\")\\n",
        "    print(\"If you just restarted the runtime, remember to run all preceding setup cells (Mount Drive, HF Login, Define Paths) first.\")\\n",
        "    # Exit here if model is not loaded, as Gradio would fail anyway\\n",
        "    # This will prevent the Gradio UI from trying to launch if the model isn't ready.\\n",
        "    raise RuntimeError(\"Fine-tuned model is not loaded. Cannot launch Gradio UI.\")\\n",
        "\\n",
        "\\n",
        "def generate_sketch_ui(poem_text, num_variations=1, guidance_scale=7.5, seed_input=-1):\\n",
        "    \"\"\"\\n",
        "    Generates sketches based on a poem using the loaded Stable Diffusion pipeline.\\n",
        "    \"\"\"\\n",
        "    if not poem_text.strip():\\n",
        "        # Return empty images if no poem is provided\\n",
        "        return [None, None, None] \\n",
        "\\n",
        "    # Generate the base and negative prompts from the poem using the function defined in Cell 2\\n",
        "    prompt, negative_prompt = analyze_poem_for_prompt(poem_text)\\n",
        "\\n",
        "    generated_images = []\\n",
        "    \\n",
        "    # Determine the seed(s) for generation\\n",
        "    # If seed_input is -1, generate random seeds for each variation.\\n",
        "    # Otherwise, use the provided seed and increment for each variation to get reproducible variations.\\n",
        "    initial_seed = seed_input if seed_input != -1 else random.randint(0, 1000000)\\n",
        "\\n",
        "    for i in range(num_variations):\\n",
        "        current_seed = initial_seed + i\\n",
        "        # Create a torch generator for reproducibility\\n",
        "        generator = torch.Generator(\"cuda\").manual_seed(current_seed)\\n",
        "        \\n",
        "        print(f\"\\n--- Generating Variation {i+1} ---\")\\n",
        "        print(f\"Seed: {current_seed}\")\\n",
        "        print(f\"Positive Prompt: {prompt}\")\\n",
        "        print(f\"Negative Prompt: {negative_prompt}\")\\n",
        "\\n",
        "        try:\\n",
        "            # Generate the image using your fine-tuned pipeline\\n",
        "            image = pipe(\\n",
        "                prompt,\\n",
        "                negative_prompt=negative_prompt,\\n",
        "                num_inference_steps=50, # 50 steps is a good balance for quality/speed\\n",
        "                guidance_scale=guidance_scale,\\n",
        "                generator=generator\\n",
        "            ).images[0]\\n",
        "            generated_images.append(image)\\n",
        "        except Exception as e:\\n",
        "            print(f\"Error generating image for variation {i+1}: {e}\")\\n",
        "            generated_images.append(None) # Append None if generation fails for a variation\\n",
        "\\n",
        "    # Gradio requires a fixed number of outputs. Pad with None if fewer than 3 variations were requested.\\n",
        "    while len(generated_images) < 3:\\n",
        "        generated_images.append(None)\\n",
        "            \\n",
        "    return generated_images[0], generated_images[1], generated_images[2]\\n",
        "\\n",
        "\\n",
        "# --- Gradio Interface Setup ---\\n",
        "print(\"\\nLaunching Gradio UI...\")\\n",
        "with gr.Blocks() as demo:\\n",
        "    gr.Markdown(\"# 🎨 Poem to Sketch AI Agent ✍️\")\\n",
        "    gr.Markdown(\"Input a poem, and the AI will generate unique, abstract sketches in your custom artistic style!\")\\n",
        "\\n",
        "    with gr.Row():\\n",
        "        with gr.Column():\\n",
        "            poem_input = gr.Textbox(\\n",
        "                label=\"✍️ Enter your poem here:\",\\n",
        "                lines=8,\\n",
        "                placeholder=\"Example:\\n'The ancient oak, a silent sentinel, \\nRooted deep in earth\\'s forgotten lore. \\nIts branches stretch, a twisted, gnarled farewell, \\nTo sunlit dreams that visit nevermore.'\"\\n",
        "            )\\n",
        "            num_variations_slider = gr.Slider(\\n",
        "                minimum=1,\\n",
        "                maximum=3,\\n",
        "                step=1,\\n",
        "                value=1,\\n",
        "                label=\"🖼️ Number of Sketch Variations\"\\n",
        "            )\\n",
        "            guidance_scale_slider = gr.Slider(\\n",
        "                minimum=5.0,\\n",
        "                maximum=15.0,\\n",
        "                step=0.5,\\n",
        "                value=7.5,\\n",
        "                label=\"💡 Guidance Scale (How strictly to follow the poem\\'s mood & style)\"\\n",
        "            )\\n",
        "            seed_input = gr.Number(\\n",
        "                label=\"🎲 Seed (-1 for random, helps with reproducibility)\",\\n",
        "                value=-1,\\n",
        "                step=1,\\n",
        "                precision=0\\n",
        "            )\\n",
        "            generate_button = gr.Button(\"✨ Generate Sketches!\")\\n",
        "\\n",
        "        with gr.Column():\\n",
        "            output_image_1 = gr.Image(label=\"Generated Sketch 1\", width=512, height=512)\\n",
        "            output_image_2 = gr.Image(label=\"Generated Sketch 2\", width=512, height=512)\\n",
        "            output_image_3 = gr.Image(label=\"Generated Sketch 3\", width=512, height=512)\\n",
        "\\n",
        "    # Bind the button click to the generation function\\n",
        "    generate_button.click(\\n",
        "        fn=generate_sketch_ui,\\n",
        "        inputs=[poem_input, num_variations_slider, guidance_scale_slider, seed_input],\\n",
        "        outputs=[output_image_1, output_image_2, output_image_3]\\n",
        "    )\\n",
        "\\n",
        "# Launch the Gradio app. share=True generates a public, temporary URL.\\n",
        "demo.launch(share=True, debug=True)\\n",
        "\\n",
        "print(\"\\n--- Gradio UI Launched! ---\")\\n",
        "print(\"Look for the public URL (usually ending in '.gradio.live') in the output above.\")\\n",
        "print(\"Click on it to open your interactive Poem-to-Sketch AI agent in a new tab.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
