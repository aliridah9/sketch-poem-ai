{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEzPU-YUyFSx",
        "outputId": "d3eb916f-9b26-4ae3-f801-461d935b51dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Vo0s87aygMJ",
        "outputId": "134c6c30-5002-40eb-f0f9-467796e8406d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uninstalling all potentially conflicting packages...\n",
            "\u001b[33mWARNING: Skipping xformers as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling PyTorch, torchvision, and torchaudio (latest cu121)...\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.5/780.5 MB\u001b[0m \u001b[31m808.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m829.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "timm 1.0.15 requires huggingface_hub, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mInstalling xformers (latest compatible)...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling bitsandbytes...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling latest diffusers, transformers, accelerate, peft, huggingface_hub...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m92.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.1/411.1 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m512.1/512.1 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling other dependencies...\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.3/323.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "--- ALL INSTALLATIONS ATTEMPTED ---\n",
            "Please RESTART RUNTIME (Runtime > Restart runtime) and then run subsequent cells.\n"
          ]
        }
      ],
      "source": [
        "# --- REVISED INSTALLATION (Attempt 14: Trying Latest Compatible Versions) ---\n",
        "\n",
        "# 1. Uninstall *all* potentially conflicting packages.\n",
        "print(\"Uninstalling all potentially conflicting packages...\")\n",
        "!pip uninstall -y -q torch torchvision torchaudio xformers bitsandbytes diffusers transformers accelerate sentence-transformers peft huggingface_hub\n",
        "\n",
        "# 2. Install PyTorch with the latest available CUDA 12.1 compatible version\n",
        "#    Let pip resolve to the highest available that matches Colab's CUDA\n",
        "print(\"Installing PyTorch, torchvision, and torchaudio (latest cu121)...\")\n",
        "!pip install -qqq torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# 3. Install xformers (latest compatible with PyTorch installed above)\n",
        "print(\"Installing xformers (latest compatible)...\")\n",
        "!pip install -qqq xformers --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# 4. Install bitsandbytes\n",
        "print(\"Installing bitsandbytes...\")\n",
        "!pip install -qqq bitsandbytes\n",
        "\n",
        "# 5. Install the very latest diffusers, transformers, accelerate, peft, huggingface_hub\n",
        "#    This assumes they are now compatible with each other and with the latest PyTorch cu121.\n",
        "print(\"Installing latest diffusers, transformers, accelerate, peft, huggingface_hub...\")\n",
        "!pip install -qqq diffusers transformers accelerate peft huggingface_hub\n",
        "\n",
        "# 6. Install other dependencies\n",
        "print(\"Installing other dependencies...\")\n",
        "!pip install -qqq Pillow numpy matplotlib scikit-learn opencv-python gradio controlnet_aux python-dotenv\n",
        "\n",
        "print(\"\\n--- ALL INSTALLATIONS ATTEMPTED ---\")\n",
        "print(\"Please RESTART RUNTIME (Runtime > Restart runtime) and then run subsequent cells.\")\n",
        "\n",
        "# ... existing code ...\n",
        "\n",
        "# --- Hugging Face Login Cell ---\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Get token from environment variable\n",
        "HUGGINGFACE_TOKEN = os.getenv('HUGGINGFACE_TOKEN')\n",
        "if not HUGGINGFACE_TOKEN:\n",
        "    raise ValueError(\"HUGGINGFACE_TOKEN not found in .env file\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(token=HUGGINGFACE_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-fsuQg2UzWm-"
      },
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Get token from environment variable\n",
        "HUGGINGFACE_TOKEN = os.getenv('HUGGINGFACE_TOKEN')\n",
        "if not HUGGINGFACE_TOKEN:\n",
        "    raise ValueError(\"HUGGINGFACE_TOKEN not found in .env file\")\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(token=HUGGINGFACE_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsbQT23dzsvT",
        "outputId": "5623e822-c045-4f1a-8945-be35bb852a94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Paths defined:\n",
            "  Instance Data: /content/drive/MyDrive/mysketches/examples of doodles\n",
            "  Output: /content/drive/MyDrive/SketchPoemAI/finetuned_model\n"
          ]
        }
      ],
      "source": [
        "import os # Make sure os is imported here\n",
        "\n",
        "# Define paths\n",
        "output_dir = \"/content/drive/MyDrive/SketchPoemAI/finetuned_model\"\n",
        "instance_data_dir = \"/content/drive/MyDrive/mysketches/examples of doodles\" # Your sketch directory\n",
        "instance_prompt = \"a sksart sketch\" # Your unique style token + class prompt\n",
        "class_data_dir = \"/content/drive/MyDrive/SketchPoemAI/class_images\" # For regularization images\n",
        "class_prompt = \"a sketch\" # General class prompt\n",
        "\n",
        "# Create directories (if not already created)\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "os.makedirs(class_data_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Paths defined:\\n  Instance Data: {instance_data_dir}\\n  Output: {output_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZgDbf1Ez1EZ"
      },
      "outputs": [],
      "source": [
        "# --- Optional: BLIP Automated Captioning Cell ---\n",
        "# --- I run it previosuly and it generated text files for me , so no need to run it .\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# No need to redefine instance_data_dir here if it's defined in the previous cell.\n",
        "# Just make sure the previous cell has been run!\n",
        "\n",
        "print(\"Loading BLIP model for captioning...\")\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(\"cuda\")\n",
        "print(\"BLIP model loaded.\")\n",
        "\n",
        "print(f\"\\nStarting captioning process for images in: {instance_data_dir}\")\n",
        "processed_count = 0\n",
        "skipped_count = 0\n",
        "error_count = 0\n",
        "\n",
        "# Loop through all files in your sketch directory\n",
        "for filename in os.listdir(instance_data_dir):\n",
        "    # ... (rest of your BLIP code remains the same) ...\n",
        "    if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.webp')):\n",
        "        image_path = os.path.join(instance_data_dir, filename)\n",
        "        text_path = os.path.join(instance_data_dir, os.path.splitext(filename)[0] + \".txt\")\n",
        "\n",
        "        if os.path.exists(text_path):\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            raw_image = Image.open(image_path).convert(\"RGB\")\n",
        "            inputs = processor(raw_image, return_tensors=\"pt\").to(\"cuda\")\n",
        "            out = model.generate(**inputs, max_length=50, num_beams=5)\n",
        "            caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "            final_caption = f\"{instance_prompt}, {caption}\"\n",
        "\n",
        "            with open(text_path, \"w\") as f:\n",
        "                f.write(final_caption)\n",
        "\n",
        "            print(f\"Caption for {filename}: {final_caption}\")\n",
        "            processed_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {e}\")\n",
        "            error_count += 1\n",
        "\n",
        "del model\n",
        "del processor\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Cleared GPU cache.\")\n",
        "\n",
        "print(f\"\\n--- Captioning Summary ---\")\n",
        "print(f\"Processed: {processed_count} images\")\n",
        "print(f\"Skipped (caption already existed): {skipped_count} images\")\n",
        "print(f\"Errors: {error_count} images\")\n",
        "print(\"\\nRemember to manually review and refine the generated captions for accuracy and style adherence!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCHX1L9lz1zv",
        "outputId": "7a55f444-965d-4b66-beb1-21b160039dca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training with runwayml/stable-diffusion-v1-5 and LoRA...\n",
            "Dataset path: /content/drive/MyDrive/mysketches/examples of doodles\n",
            "Instance prompt: 'a sksart sketch'\n",
            "Output directory: /content/drive/MyDrive/SketchPoemAI/finetuned_model\n",
            "Max training steps: 5000\n",
            "Attempting to download train_dreambooth_lora.py script to: /content/train_dreambooth_lora.py\n",
            "From URL: https://raw.githubusercontent.com/huggingface/diffusers/v0.27.1/examples/dreambooth/train_dreambooth_lora.py\n",
            "/content\n",
            "Current working directory before download: /content\n",
            "Successfully downloaded script: /content/train_dreambooth_lora.py\n",
            "Current working directory for accelerate launch: /content\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `1`\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/train_dreambooth_lora.py\", line 46, in <module>\n",
            "    import diffusers\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/diffusers/__init__.py\", line 5, in <module>\n",
            "    from .utils import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/diffusers/utils/__init__.py\", line 38, in <module>\n",
            "    from .dynamic_modules_utils import get_class_from_dynamic_module\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/diffusers/utils/dynamic_modules_utils.py\", line 28, in <module>\n",
            "    from huggingface_hub import cached_download, hf_hub_download, model_info\n",
            "ImportError: cannot import name 'cached_download' from 'huggingface_hub' (/usr/local/lib/python3.11/dist-packages/huggingface_hub/__init__.py)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/accelerate_cli.py\", line 50, in main\n",
            "    args.func(args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/launch.py\", line 1198, in launch_command\n",
            "    simple_launcher(args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/accelerate/commands/launch.py\", line 785, in simple_launcher\n",
            "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
            "subprocess.CalledProcessError: Command '['/usr/bin/python3', '/content/train_dreambooth_lora.py', '--pretrained_model_name_or_path=runwayml/stable-diffusion-v1-5', '--instance_data_dir=/content/drive/MyDrive/mysketches/examples of doodles', '--instance_prompt=a sksart sketch', '--output_dir=/content/drive/MyDrive/SketchPoemAI/finetuned_model', '--mixed_precision=fp16', '--resolution=512', '--train_batch_size=1', '--gradient_accumulation_steps=1', '--learning_rate=0.0001', '--lr_scheduler=cosine', '--lr_warmup_steps=100', '--max_train_steps=5000', '--checkpointing_steps=1000', '--seed=42', '--with_prior_preservation', '--class_data_dir=/content/drive/MyDrive/SketchPoemAI/class_images', '--class_prompt=a sketch', '--num_class_images=200', '--validation_prompt=a sksart sketch of a tree']' returned non-zero exit status 1.\n",
            "\n",
            "Model training complete. LoRA weights saved to: /content/drive/MyDrive/SketchPoemAI/finetuned_model\n",
            "You can now proceed to the Inference Pipeline cell to generate sketches from poems.\n"
          ]
        }
      ],
      "source": [
        "# --- Training Pipeline Cell (Final attempt to get it running - FULL CODE) ---\n",
        "\n",
        "import os # Ensure os is imported here for file system operations\n",
        "\n",
        "# --- Training Parameters ---\n",
        "model_name = \"runwayml/stable-diffusion-v1-5\" # Base model\n",
        "resolution = 512\n",
        "train_batch_size = 1\n",
        "gradient_accumulation_steps = 1\n",
        "learning_rate = 1e-4\n",
        "lr_scheduler = \"cosine\"\n",
        "lr_warmup_steps = 100\n",
        "max_train_steps = 5000\n",
        "save_steps = 1000\n",
        "seed = 42\n",
        "\n",
        "# These variables should be defined from your \"Define Paths & Create Directories\" cell (Cell [5]).\n",
        "# They are included here as comments just for reference.\n",
        "\n",
        "print(f\"\\nStarting training with {model_name} and LoRA...\")\n",
        "print(f\"Dataset path: {instance_data_dir}\") # This variable should now be defined\n",
        "print(f\"Instance prompt: '{instance_prompt}'\") # This variable should now be defined\n",
        "print(f\"Output directory: {output_dir}\") # This variable should now be defined\n",
        "print(f\"Max training steps: {max_train_steps}\")\n",
        "\n",
        "\n",
        "# --- CRITICAL: Ensure the script downloads and is found ---\n",
        "script_url = \"https://raw.githubusercontent.com/huggingface/diffusers/v0.27.1/examples/dreambooth/train_dreambooth_lora.py\"\n",
        "script_local_path = \"/content/train_dreambooth_lora.py\" # The desired path for the downloaded script\n",
        "\n",
        "print(f\"Attempting to download train_dreambooth_lora.py script to: {script_local_path}\")\n",
        "print(f\"From URL: {script_url}\")\n",
        "\n",
        "# Ensure we are in /content directory before downloading, so the script goes to the correct place\n",
        "%cd /content\n",
        "print(f\"Current working directory before download: {os.getcwd()}\")\n",
        "\n",
        "# Use !wget to download the script. -q for quiet, -O to specify output file.\n",
        "!wget -q -O {script_local_path} {script_url}\n",
        "\n",
        "# Verify the script was downloaded\n",
        "if os.path.exists(script_local_path):\n",
        "    print(f\"Successfully downloaded script: {script_local_path}\")\n",
        "else:\n",
        "    print(f\"ERROR: Script not found after download attempt at {script_local_path}. Please check the URL or your network connection.\")\n",
        "    raise FileNotFoundError(f\"Training script not found: {script_local_path}\")\n",
        "\n",
        "print(f\"Current working directory for accelerate launch: {os.getcwd()}\")\n",
        "\n",
        "\n",
        "# --- Command to run the DreamBooth LoRA training script ---\n",
        "# We directly execute the downloaded script using its full path ({script_local_path})\n",
        "!accelerate launch {script_local_path} \\\n",
        "    --pretrained_model_name_or_path=\"$model_name\" \\\n",
        "    --instance_data_dir=\"$instance_data_dir\" \\\n",
        "    --instance_prompt=\"$instance_prompt\" \\\n",
        "    --output_dir=\"$output_dir\" \\\n",
        "    --mixed_precision=\"fp16\" \\\n",
        "    --resolution=$resolution \\\n",
        "    --train_batch_size=$train_batch_size \\\n",
        "    --gradient_accumulation_steps=$gradient_accumulation_steps \\\n",
        "    --learning_rate=$learning_rate \\\n",
        "    --lr_scheduler=$lr_scheduler \\\n",
        "    --lr_warmup_steps=$lr_warmup_steps \\\n",
        "    --max_train_steps=$max_train_steps \\\n",
        "    --checkpointing_steps=$save_steps \\\n",
        "    --seed=$seed \\\n",
        "    --with_prior_preservation --class_data_dir=\"$class_data_dir\" --class_prompt=\"$class_prompt\" \\\n",
        "    --num_class_images=200 \\\n",
        "    --validation_prompt=\"a sksart sketch of a tree\" \\\n",
        "\n",
        "print(f\"\\nModel training complete. LoRA weights saved to: {output_dir}\")\n",
        "print(\"You can now proceed to the Inference Pipeline cell to generate sketches from poems.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXPeMbKM_WRX",
        "outputId": "1216636f-3304-423d-f452-55acd969b8ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No checkpoint folders found in /content/drive/MyDrive/SketchPoemAI/finetuned_model.\n",
            "ERROR: LoRA model weights not found. Cannot proceed with inference.\n",
            "Please ensure your training process completed and saved the weights to the correct 'output_dir'.\n"
          ]
        }
      ],
      "source": [
        "# --- Inference Pipeline: Cell 1 - Load the Fine-tuned Model ---\n",
        "\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Define the base model (should match the one you used for training)\n",
        "base_model_name = \"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "# Define the path to your fine-tuned LoRA weights\n",
        "# The training script saves LoRA weights in a subfolder like 'checkpoint-LAST_CHECKPOINT_NUMBER'\n",
        "# This code will automatically find the latest checkpoint.\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/SketchPoemAI/finetuned_model\" # This variable should be defined from your earlier 'Define Paths' cell\n",
        "\n",
        "latest_checkpoint_path = None\n",
        "if os.path.exists(output_dir):\n",
        "    # Find all checkpoint folders\n",
        "    checkpoint_folders = [f for f in os.listdir(output_dir) if f.startswith('checkpoint-')]\n",
        "    if checkpoint_folders:\n",
        "        # Get the highest numbered checkpoint folder (e.g., checkpoint-5000)\n",
        "        latest_checkpoint_folder = max(checkpoint_folders, key=lambda x: int(x.split('-')[1]))\n",
        "        latest_checkpoint_path = os.path.join(output_dir, latest_checkpoint_folder)\n",
        "        # Construct the full path to the weights file\n",
        "        lora_model_path = os.path.join(latest_checkpoint_path, \"pytorch_lora_weights.safetensors\")\n",
        "    else:\n",
        "        print(f\"No checkpoint folders found in {output_dir}.\")\n",
        "        lora_model_path = None\n",
        "else:\n",
        "    print(f\"Output directory {output_dir} does not exist. Please check your path.\")\n",
        "    lora_model_path = None\n",
        "\n",
        "# Proceed only if the LoRA weights path is valid\n",
        "if lora_model_path and os.path.exists(lora_model_path):\n",
        "    print(f\"Found LoRA weights at: {lora_model_path}\")\n",
        "    print(\"Loading base Stable Diffusion pipeline...\")\n",
        "    # Load the base model; it will download if not cached\n",
        "    pipe = StableDiffusionPipeline.from_pretrained(base_model_name, torch_dtype=torch.float16)\n",
        "\n",
        "    print(\"Loading LoRA weights into the pipeline...\")\n",
        "    # Load your custom LoRA weights\n",
        "    pipe.load_lora_weights(os.path.dirname(lora_model_path), weight_name=os.path.basename(lora_model_path))\n",
        "\n",
        "    # Move the entire model to the GPU for inference\n",
        "    pipe.to(\"cuda\")\n",
        "    print(\"Fine-tuned model loaded successfully for inference!\")\n",
        "else:\n",
        "    print(\"ERROR: LoRA model weights not found. Cannot proceed with inference.\")\n",
        "    print(\"Please ensure your training process completed and saved the weights to the correct 'output_dir'.\")\n",
        "    pipe = None # Set pipe to None if loading failed, to prevent further errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqHHOi89Akpa",
        "outputId": "565c0c1d-d07c-47e2-f0da-df52d0841e50"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking contents of: /content/drive/MyDrive/SketchPoemAI/finetuned_model\n",
            "\n",
            "Directory is empty.\n",
            "\n",
            "Please verify the actual contents of your Google Drive.\n"
          ]
        }
      ],
      "source": [
        "# --- Debugging: Check Contents of Output Directory ---\n",
        "import os\n",
        "\n",
        "output_dir = \"/content/drive/MyDrive/SketchPoemAI/finetuned_model\"\n",
        "\n",
        "print(f\"Checking contents of: {output_dir}\")\n",
        "if os.path.exists(output_dir):\n",
        "    contents = os.listdir(output_dir)\n",
        "    if contents:\n",
        "        print(\"Contents found:\")\n",
        "        for item in contents:\n",
        "            print(f\"- {item}\")\n",
        "\n",
        "        checkpoint_folders = [f for f in contents if f.startswith('checkpoint-')]\n",
        "        if checkpoint_folders:\n",
        "            print(f\"\\nDetected checkpoint folders: {checkpoint_folders}\")\n",
        "            latest_checkpoint_folder = max(checkpoint_folders, key=lambda x: int(x.split('-')[1]))\n",
        "            potential_lora_path = os.path.join(output_dir, latest_checkpoint_folder, \"pytorch_lora_weights.safetensors\")\n",
        "            print(f\"Looking for LoRA weights at: {potential_lora_path}\")\n",
        "            if os.path.exists(potential_lora_path):\n",
        "                print(\"\\nSUCCESS: LoRA weights file EXISTS at this path!\")\n",
        "                print(\"You can proceed to run the 'Inference Pipeline: Cell 1 - Load the Fine-tuned Model' again.\")\n",
        "            else:\n",
        "                print(\"\\nERROR: LoRA weights file DOES NOT EXIST at the expected path inside the checkpoint folder.\")\n",
        "                print(\"Double-check the file name inside your checkpoint folder.\")\n",
        "        else:\n",
        "            print(\"\\nERROR: No folders starting with 'checkpoint-' found.\")\n",
        "    else:\n",
        "        print(\"\\nDirectory is empty.\")\n",
        "else:\n",
        "    print(f\"\\nERROR: Directory {output_dir} does NOT exist.\")\n",
        "\n",
        "print(\"\\nPlease verify the actual contents of your Google Drive.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
